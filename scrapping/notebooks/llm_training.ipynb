{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from collections import Counter\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Vocabulary Size: 19739\n"
     ]
    }
   ],
   "source": [
    "# Load the JSON data\n",
    "with open('../cleaned_data/cleaned_articles.json', 'r', encoding='utf-8') as file:\n",
    "    articles = json.load(file)\n",
    "\n",
    "# Flatten the data into a list of texts\n",
    "texts = [article['content'] for article in articles] + [article['title'] for article in articles]\n",
    "\n",
    "# Tokenize and count word frequencies\n",
    "word_counter = Counter()\n",
    "for text in texts:\n",
    "    word_counter.update(text.split())\n",
    "\n",
    "# Sort words by frequency\n",
    "most_common_words = word_counter.most_common()\n",
    "\n",
    "# Determine vocab size\n",
    "desired_vocab_size = len(most_common_words)\n",
    "print(f\"Optimal Vocabulary Size: {desired_vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "BPE Tokenizer trained and saved.\n",
      "Full tokenized data saved to: ../cleaned_data/tokenized_articles.pt\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "import json\n",
    "\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "import torch\n",
    "import json\n",
    "\n",
    "# Directory paths\n",
    "data_files = [\"../cleaned_data/cleaned_articles.json\"]\n",
    "vocab_size = 19739  # Based on your earlier calculation\n",
    "special_tokens = [\"[CLS]\", \"[SEP]\", \"[PAD]\", \"[UNK]\", \"[MASK]\"]\n",
    "\n",
    "# Load the data\n",
    "with open(data_files[0], 'r', encoding='utf-8') as file:\n",
    "    articles = json.load(file)\n",
    "\n",
    "# Extract text content\n",
    "texts = [article['content'] for article in articles]\n",
    "with open(\"texts.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for text in texts:\n",
    "        f.write(text + \"\\n\")\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer(lowercase=False)\n",
    "tokenizer.train(files=[\"texts.txt\"], vocab_size=vocab_size, special_tokens=special_tokens)\n",
    "tokenizer.save(\"../cleaned_data/tokenizer.json\")\n",
    "print(\"BPE Tokenizer trained and saved.\")\n",
    "\n",
    "# Tokenize the texts\n",
    "tokenized_texts = [tokenizer.encode(text).ids for text in texts]\n",
    "\n",
    "# Save token IDs\n",
    "tokenized_data_path_pt = \"../cleaned_data/tokenized_articles.pt\"\n",
    "torch.save(tokenized_texts, tokenized_data_path_pt)\n",
    "print(f\"Full tokenized data saved to: {tokenized_data_path_pt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tokenized_articles.pt', 'cleaned_data.csv', 'merges.txt', 'vocab.txt', 'cleaned_articles.json']\n",
      "['tokenized_articles.pt', 'cleaned_data.csv', 'merges.txt', 'vocab.txt', 'cleaned_articles.json']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Check contents before saving\n",
    "print(os.listdir('../cleaned_data/'))\n",
    "\n",
    "# Save the tokenizer again\n",
    "tokenizer.save_model(\"../cleaned_data/\")\n",
    "\n",
    "# Check contents after saving\n",
    "print(os.listdir('../cleaned_data/'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xm/3ct03h_d797_ld5z2gg5nmsc0000gn/T/ipykernel_37912/3275527867.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  token_ids = torch.load('../cleaned_data/tokenized_articles.pt')\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, token_ids, max_length=512):\n",
    "        self.token_ids = token_ids\n",
    "        self.max_length = max_length\n",
    "        self.labels = [0] * len(token_ids)  # Use dummy labels (0)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.token_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.token_ids[idx]\n",
    "        input_ids = torch.tensor(tokens, dtype=torch.long)\n",
    "        # Pad/truncate the input_ids to max_length\n",
    "        if len(input_ids) > self.max_length:\n",
    "            input_ids = input_ids[:self.max_length]\n",
    "        else:\n",
    "            padding_length = self.max_length - len(input_ids)\n",
    "            input_ids = torch.cat([input_ids, torch.zeros(padding_length, dtype=torch.long)])\n",
    "        \n",
    "        attention_mask = (input_ids != 0).long()  # Create attention mask\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': label\n",
    "        }\n",
    "\n",
    "# Load token IDs\n",
    "token_ids = torch.load('../cleaned_data/tokenized_articles.pt')\n",
    "dataset = TextDataset(token_ids)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/Library/Caches/pypoetry/virtualenvs/adlamandnko-KnSx1iNo-py3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW\n",
    "\n",
    "# Load pre-trained model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/Library/Caches/pypoetry/virtualenvs/adlamandnko-KnSx1iNo-py3.12/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.0036, grad_fn=<NllLossBackward0>), logits=tensor([[ 2.9056, -2.9931],\n",
      "        [ 2.6866, -2.7539],\n",
      "        [ 2.9378, -2.8211],\n",
      "        [ 2.7765, -2.9682],\n",
      "        [ 2.6876, -2.7863],\n",
      "        [ 2.9094, -2.8773],\n",
      "        [ 2.3587, -2.7742],\n",
      "        [ 2.9675, -3.1159]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Epoch 1/3 completed. Loss: 0.003602477489039302\n",
      "SequenceClassifierOutput(loss=tensor(0.0032, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0805, -3.1363],\n",
      "        [ 2.8175, -3.0067],\n",
      "        [ 2.9313, -3.2063],\n",
      "        [ 2.9199, -2.9347],\n",
      "        [ 2.7873, -3.0725],\n",
      "        [ 2.5048, -2.7379],\n",
      "        [ 2.7103, -3.0190],\n",
      "        [ 2.7033, -2.7771]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Epoch 2/3 completed. Loss: 0.0031860042363405228\n",
      "SequenceClassifierOutput(loss=tensor(0.0021, grad_fn=<NllLossBackward0>), logits=tensor([[ 3.0636, -3.3252],\n",
      "        [ 3.2086, -3.1514],\n",
      "        [ 3.1547, -2.9644],\n",
      "        [ 3.3677, -3.1081],\n",
      "        [ 3.1973, -2.6898],\n",
      "        [ 3.0286, -2.7138],\n",
      "        [ 3.3426, -3.1658],\n",
      "        [ 3.1562, -3.0327]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Epoch 3/3 completed. Loss: 0.0020820824429392815\n"
     ]
    }
   ],
   "source": [
    "# Setup optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "def train(model, dataloader, optimizer, device, epochs=3):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            print(outputs)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            break\n",
    "        print(f'Epoch {epoch + 1}/{epochs} completed. Loss: {loss.item()}')\n",
    "\n",
    "# Train the model\n",
    "train(model, dataloader, optimizer, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[['[CLS]', '𞤤𞤫𞤧𞤯𞤫', '𞤸𞤭𞥅𞤪𞤲𞤢', '##𞤲', '##𞤳𞤮𞥅', '##𞤶𞤫', '(', '𞤮', '##𞤳𞤧', '##𞤭𞤣', '##𞤢𞤲', ')', '𞥑𞥕', '𞤫', '𞤤𞤫𞤴', '𞤳𞤮𞤲𞤺𞤮𞤤', '𞤢𞤥𞤫𞤪𞤭𞤳', '𞤳𞤢𞤻𞤵𞤲', '𞤫', '𞤮', '##𞤧𞤼𞤢', '##𞤪𞤢𞤤', '##𞤭𞤴𞤢', '𞤽𞤭𞤻𞤭𞥅', '𞤷𞤢', '##𞤴𞤲', '##𞤢', '𞤧𞤢𞤦𞤵', '𞤱𞤢𞥄𞤧𞤵𞤺𞤮𞤤', '𞤥𞤢𞤩𞥆𞤫', '𞤼𞤮𞤼𞥆𞤵𞤺𞤮𞤤', '𞤸𞤢𞤹𞥆𞤫𞥅𞤶𞤭', '𞤱𞤭', '##𞤺𞤵', '##𞥅', '##𞤪', '##𞤩𞤫', '𞤩𞤫𞤲', '(', '𞤶𞤵𞥅𞤤𞤩𞤫', '𞤶𞤫𞤴𞥆𞤢𞤩𞤫', '𞤸𞤭𞥅𞤪𞤲𞤢𞥄𞤲𞤺𞤫', '-', '𞤲𞤢𞤲𞤮', '𞤷𞤢', '##𞤴𞤲', '##𞤢', '𞤫', '𞤤𞤢𞤪𞤢𞤤', '𞤭𞤲𞥆𞤫𞤼𞤫𞥅𞤲𞤺𞤢𞤤', '𞥃', '##𞤭𞤲', '##𞥁', '##𞤭𞤴𞤢𞤲', ')', '.', '𞤷𞤢', '##𞤴', '##𞤦𞤢', '𞤶𞤢𞥄𞤦𞤭𞥅', '𞤩𞤫', '⹁', '𞤸𞤮𞤤', '𞤳𞤮', '𞤱𞤭𞥅𞤴𞤵𞤯𞤮𞤲', '𞤫', '𞤳𞤮', '𞤱𞤮𞤲𞤭', '𞤫', '𞤬𞤫𞤰𞥆𞤵𞤣𞤫', '𞤦𞤢𞤽𞥆𞤫', '𞤺𞤢𞥄', '##𞥁', '##𞥆𞤢', '؟', '𞤲𞤭𞤤𞥆𞤢𞥄𞤶𞤮', '𞤷𞤢', '##𞤴𞤲', '##𞤢', '𞤳𞤢', '𞤲𞤶𞤵𞤩𞥆𞤵𞤣𞤭', '𞤺𞤫𞤲', '##𞤯𞤫', '𞤬𞤮𞤼', '##𞥆𞤵𞤯𞤫', '(', 'on', '##u', '–', 'un', ')', '𞤸𞤮𞤤𞥆𞤭', '𞤱𞤮𞤲𞤣𞤫𞤥𞤢', ':', '«', '𞤥𞤵𞤪𞤢𞥄𞤣𞤵', '𞤬𞤭𞥅', '𞤸𞤢𞤹𞥆𞤫𞥅𞤶𞤭', '𞤲𞤫𞤯𞥆𞤢𞤲𞤳𞤫', '𞤲𞤣𞤵', '𞤺𞤮𞥅𞤥𞤵', '𞤺𞤫𞤲', '##𞤯𞤫', '𞤬𞤮𞤼', '##𞥆𞤵𞤯𞤫', '𞤯𞤫𞤲', '𞤬𞤵𞤼𞤭', '𞤸𞤭𞥅𞤤𞤲𞤢𞥄𞤣𞤫', '𞤸𞤭𞤳𞥆𞤢', '⹁', '𞤧𞤭𞤳𞥆𞤫', '𞤢𞤤𞤢𞥄', '𞤳𞤮', '𞤺𞤢𞥄', '##𞥁', '##𞥆𞤢', '.', '»', '𞤮', '𞤩𞤫𞤴𞤣𞤭', '𞤳𞤢𞤣𞤭', ':', '«', '𞤯𞤮𞥅', '𞤴𞤮', '𞤮', '##𞤧𞤼𞤢', '##𞤪𞤢𞤤', '##𞤭𞤴𞤢', '𞤫', '𞤢𞤥𞤫𞤪𞤭𞤳', '𞤫', '𞤴𞤮𞤺𞤢', '𞤫', '𞤤𞤫𞤴𞤯𞤫', '𞤺𞤮𞤮', '𞤬𞤮𞤴𞤺𞤭𞤼𞤵', '𞤲𞤺𞤵𞥅𞤪𞤲𞤣𞤢𞤥', '𞤶𞤢𞤴𞤲𞤺𞤫', '𞤯𞤢𞤥', '𞤨𞤢', '##𞤤𞤫𞤧', '##𞤼𞤭𞥅𞤲', '𞤱𞤵𞥅𞤪𞤭', '𞤫', '𞤥𞤵𞥅𞤯𞤵𞤲', '⹁', '𞤸𞤭𞤩𞤫', '𞤶𞤮𞤳𞥆𞤭', '𞤫', '𞤬𞤵𞤪𞤼𞤭𞤲𞤺𞤮𞤤', '𞤼𞤢𞤰𞤢𞥄𞤤𞤫', '𞤫', '𞤨𞤫𞤲𞤢𞥄𞤤𞤫', '𞤸𞤮𞥅𞤪𞤫', '𞤣𞤭𞥅𞤱𞤢𞤤', '𞥃', '##𞤭𞤲', '##𞥁', '##𞤭𞤴𞤢𞤲', '𞤳𞤢', '𞤩𞤵𞥅𞤩𞤼𞤵', '𞤫', '𞤳𞤵𞤧𞤢𞤤', '𞤼𞤢𞤦𞤭𞤼𞤭', '.', '»', '[SEP]'], ['[CLS]', '𞤢𞤱𞤢', '𞤧𞤭𞤳𞥆𞤢𞤲𞤮𞥅𞤯𞤮', '𞤮𞤲', '𞤥𞤢𞤴𞤭𞥅', '𞤢𞤪𞤼𞤭𞥅', '.', '𞤨𞤮𞤤', '𞤦𞤭', '##𞤴𞤢', '⹁', '𞤸𞤮𞥅𞤪𞤫𞥅𞤶𞤮', '𞤤𞤫𞤧𞤣𞤭', '𞤳𞤢', '##𞤥𞤢𞤪', '##𞤵𞤲', '𞤲𞤣𞤭𞤲', '⹁', '𞤪𞤵𞤼𞥆𞤭𞤳𞤫', '𞤼𞤮', '𞤤𞤢𞥄𞤥𞤮𞤪𞤣𞤫', '𞤴𞤢𞤱', '##𞤵𞤲𞤣𞤫', '𞤸𞤢𞤲𞤳𞤭', '𞤫', '𞤢𞥄𞤩', '##𞤲𞤣𞤫', '𞤻𞤢𞤤𞤲𞤣𞤫', '𞥒𞥑', '𞤴𞤢𞤪𞤳𞤮', '.', '𞤳𞤮', '𞤯𞤵𞤥', '𞤰𞤫𞤱𞤤𞤵', '𞤲𞤺𞤫𞤲𞤣𞤭𞥅𞤪𞤵', '𞤳𞤢', '##𞤥𞤢𞤪', '##𞤵𞤲', '𞤲𞤣𞤵𞤲', '𞤸𞤮𞤤𞥆𞤭', '.', '𞤸𞤢𞤪𞤭', '𞤩𞤵𞤪𞤲𞤭𞥅', '𞤶𞤮𞤲𞤼𞤫', '𞤶𞤫𞥅𞤺𞤮𞤮', '⹁', '𞤨𞤮𞤤', '𞤦𞤭', '##𞤴𞤢', '𞤴𞤭𞤴𞤢𞥄𞤳𞤢', '𞤫', '𞤳𞤫𞤲𞤫', '.', '𞤳𞤮', '𞤻𞤢𞤤𞤲𞤣𞤫', '𞥘', '𞤧𞤭𞤤', '##𞤼𞤮', '𞥒𞥐𞥒𞥔', '𞤮', '𞤱𞤢𞤴𞤪𞤢𞤲𞤮𞥅', '𞤴𞤭𞤴𞤫𞥅𞤣𞤫', '𞤳𞤢', '𞤼𞤵𞤥𞤦𞤮𞤲𞥋𞤣𞤭𞤪𞤢𞤤', '𞤷𞤢', '##𞤴𞤲', '##𞤢', '-', '𞤢𞤬𞤪𞤭𞤳', '𞤱𞤢𞤯𞤵𞤲𞥋𞤺𞤢𞤤', '𞤦𞤫', '##𞤴', '##𞤶𞤭𞤲𞤺', '⹁', '𞤤𞤢𞥄𞤥𞤮𞤪𞤣𞤫', '𞤷𞤢', '##𞤴𞤲', '##𞤢', '.', '𞤩𞤢𞥄𞤱𞤼𞤭', '𞤯𞤵𞤥', '⹁', '𞤼𞤵𞤥𞤦𞤮𞤲𞥋𞤣𞤭𞤪𞤫', '𞤥𞤢𞤱𞤯𞤫', '𞤱𞤢𞤯𞤭𞥅', '𞤯𞤫', '𞤮', '𞤼𞤭𞤶𞥆𞤢𞤲𞤮𞥅', '𞤮', '𞤼𞤢𞤱𞤣𞤫𞤼𞤫', '⹁', '𞤱𞤢𞤲𞤮', '𞤥𞤮𞥅𞤩𞤼𞤮𞤲', '##𞥋𞤣𞤭𞤪', '##𞤢𞤤', '𞤳𞤵𞥅𞤩𞤢𞤤', '𞤺𞤫𞤲', '##𞤯𞤫', '𞤬𞤮𞤼', '##𞥆𞤵𞤯𞤫', '(', 'on', '##u', '/', 'un', ')', '⹁', '𞤳𞤢𞤻𞤵𞤲', '𞤫', '𞤼𞤵𞤥𞤦𞤮𞤲𞥋𞤣𞤭𞤪𞤢𞤤', '𞤲𞤶𞤵𞤩𞥆𞤵𞤣𞤭', '𞤱𞤭𞤲𞤣𞤫𞤪𞤫', '##𞥅', '##𞤪𞤭', '𞤸𞤢𞥄𞤤𞤭', '-', '𞤬𞤢𞤪𞤢𞤲𞤧𞤭', '𞤫𞤲', '.', '𞤲𞤣𞤫', '𞤼𞤢𞤱𞤵𞤲𞤮𞥅', '𞤮', '𞤴𞤭𞤴𞤢𞥄𞤳𞤢', '𞤫', '𞤯𞤫𞥅', '𞤼𞤵𞤥𞤦𞤮𞤲𞥋𞤣𞤭𞤪𞤫', '⹁', '𞤸𞤢𞥄𞤤𞤢𞥄𞤶𞤭', '𞤬𞤵𞤯𞥆𞤭', '𞤶𞤢𞥄𞤤𞤢𞥄𞤣𞤫', '𞤱𞤮𞤲𞤣𞤫𞤥𞤢', '𞤮𞤥𞤮', '𞤲𞤢𞥄𞤱𞤲𞤭', '𞤳𞤮', '𞤼𞤭𞥅𞤯𞤭', '.', '𞤱𞤵𞤪𞤭𞤲', '𞤱𞤮𞥅𞤣𞤭', '𞤱𞤭𞤴𞤵𞤲𞤮𞥅𞤩𞤫', '𞤮', '𞤥𞤢𞥄𞤴𞤭𞥅', '.', '𞤦𞤢𞤯𞤼𞤢𞤲𞤫', '𞤯𞤵𞤥', '⹁', '𞤤𞤢𞥄𞤥𞤵', '𞤳𞤢', '##𞤥𞤢𞤪', '##𞤵𞤲', '𞤲𞤺𞤵𞤲', '𞤼𞤮𞤽𞤭', '𞤸𞤢𞥄𞤤𞤵𞤺𞤮𞤤', '𞤳𞤮', '𞤴𞤮𞤱𞤭𞤼𞤭𞥅', '𞤫', '𞤷𞤫𞤤𞥆𞤢𞤤', '𞤨𞤮𞤤', '𞤦𞤭', '##𞤴𞤢', '𞤲𞤺𞤢𞤤', '.', '𞤨𞤮𞤤', '𞤦𞤭', '##𞤴𞤢', '𞤸𞤫𞤩𞤭𞥅', '𞤩𞤵𞤪𞤭', '𞤣𞤵𞥅𞤩𞤭', '𞥙𞥑', '.', '𞤳𞤮', '𞤻𞤢𞤤𞤲𞤣𞤫', '𞥑𞥓', '𞤷𞤮𞤤𞤼𞤮', '𞥑𞥙𞥓𞥓', '𞤮', '𞤶𞤭𞤦𞤭𞤲𞤢𞥄', '𞤫', '𞤸𞤮𞤯𞤮', '𞤱𞤭𞤴𞤫𞤼𞤫𞥅𞤲𞤺𞤮', '𞤥', '##𞤾', '##𞤮𞤥𞤫𞤳𞤢', '⹁', '𞤶𞤫𞤴𞤢𞥄𞤲𞤺𞤮', '𞤫', '𞤣𞤭𞥅𞤱𞤢𞤤', '𞤻𞤢𞥄𞤥𞤲𞤢𞤱𞤢𞤤', '𞤳𞤢', '##𞤥𞤢𞤪', '##𞤵𞤲', '𞤲𞤺𞤢𞤤', '.', '𞤮', '𞤼𞤭𞤥𞥆𞤭𞤲𞤭𞥅', '𞤣𞤵𞥅𞤩𞤭', '𞥔𞥒', '𞤫', '𞤳𞤮', '𞤬𞤢𞤱𞤭', '𞤳𞤢', '𞤤𞤢𞥄𞤥𞤵', '.', '𞤳𞤮', '𞤫', '𞤶𞤮𞤤𞤮', '𞥑𞥙𞥘𞥒', '𞤮', '𞤤𞤮𞤲𞤼𞤭𞥅', '𞤢𞤸𞤥𞤢𞤣𞤵', '𞤢𞤸', '##𞤭𞥅𞤶𞤮', '𞤸𞤮𞥅𞤪𞤫', '𞤳𞤢', '##𞤥𞤢𞤪', '##𞤵𞤲', '𞤩𞤢𞥄𞤱𞤮', '𞤮𞤲', '𞤢𞤷𞥆𞤭𞤼𞤵𞤣𞤫', '𞤤𞤢𞥄𞤥𞤵', '𞤲𞤺𞤵𞤲', '.', '𞤬𞤫𞤱𞤲𞤣𞤮', '𞤮', '𞤤𞤮𞤲𞤼𞤮𞤼𞤮𞥅', '⹁', '𞤼𞤢𞤱𞤭', '𞤳𞤮', '𞤳𞤢𞤲𞤳𞤮', '𞤤𞤢𞥄𞤼𞤭𞤲𞤮𞥅', '𞤢𞤪𞤣𞤮', '𞤶𞤢𞥄𞤺𞤮𞤪𞤢𞤤', '𞤲𞤺𞤢𞤤', '.', '𞤽𞤢𞤤𞤯𞤭𞤲𞤭', '𞤯𞤵𞥅𞤯𞤵𞤩𞤫', '𞤳𞤮', '𞤼𞤢𞤱𞤭', '𞤳𞤮𞤲', '𞤢𞤸𞤥𞤢𞤣𞤵', '𞤢𞤸', '##𞤭𞥅𞤶𞤮', '⹁', '𞤶𞤵𞥅𞤤𞤯𞤮', '𞤭𞤥𞥆𞤮𞤪𞤯𞤮', '𞤳𞤢', '𞤲𞤢𞥄𞤲𞥆𞤢𞥄𞤪𞤭', '𞤳𞤢', '##𞤥𞤢𞤪', '##𞤵𞤲', '𞤢𞤷𞥆𞤭𞤼𞤢𞤲𞤭𞥅', '𞤶𞤵𞤤𞥆𞤫𞤪𞤫', '𞤤𞤢𞥄𞤥𞤵', '𞤲𞤣𞤫𞤲', '𞤶𞤮𞤳𞥆𞤵𞤯𞤮', '𞤲𞤢𞤧𞤢𞥄𞤪𞤢𞥄𞤳𞤵', '𞤭𞤥𞥆𞤮𞤪𞤯𞤮', '𞤳𞤢', '𞤻𞤢𞥄𞤥𞤲𞤢𞥄𞤪𞤭', '.', '𞤳𞤮𞤲𞤮', '𞤸𞤮𞥅𞤤𞤢𞥄𞤪𞤫', '𞤫', '𞤱𞤫𞥅𞤤𞤣𞤭𞤺𞤢𞤤', '𞤸𞤢𞤳𞥆𞤵𞤲𞥋𞤣𞤫', '𞤥𞤢𞤩𞥆𞤫', '𞤩𞤮𞥅𞤴𞤢𞥄𞤲𞤮', '𞤩𞤢𞥄𞤱𞤮', '𞤢𞤸𞤥𞤢𞤣𞤵', '𞤢𞤸', '##𞤭𞥅𞤶𞤮', '𞤢𞤷𞥆𞤭𞤼𞤵𞤣𞤫', '𞤤𞤢𞥄𞤥𞤵', '𞤲𞤺𞤵𞤲', '.', '𞤱𞤵𞤪𞤭𞤲', '𞤼𞤭𞤥𞥆𞤢𞥄𞤤𞤭', '𞤸𞤭𞤼𞤢𞥄𞤲𞤣𞤫', '⹁', '𞤫', '𞤥𞤮𞤪', '##𞤧𞤮', '𞥑𞥙𞥘𞥓', '⹁', '𞤢𞤸', '##𞤭𞥅𞤶𞤮', '𞤣𞤮𞤺𞤭', '𞤤𞤫𞤴𞤣𞤭', '𞤲𞤣𞤭𞤲', '𞤴𞤢𞤸𞤭', '𞤬𞤢𞥄𞤦𞤼𞤭𞤲𞤮𞤴𞤭𞥅', '𞤬𞤢𞤪𞤢𞤲𞤧𞤭', '.', '𞤩𞤢𞥄𞤱𞤼𞤭', '𞤯𞤵𞤥', '⹁', '𞤨𞤮𞤤', '𞤦𞤭', '##𞤴𞤢', '𞤭𞤼𞥆𞤭', '𞤴𞤭𞤥𞤩𞤫', '𞤢𞤸', '##𞤭𞥅𞤶𞤮', '𞤩𞤫𞤲', '𞤬𞤮𞤨', '𞤳𞤢', '𞤶𞤢𞥄𞤺𞤮𞤪𞤢𞤤', '.', '𞤫', '𞤷𞤮𞤤𞤼𞤮', '𞥑𞥙𞥘𞥔', '⹁', '𞤢𞤸', '##𞤭𞥅𞤶𞤮', '𞤼𞤵𞥅𞤥𞤢𞥄', '𞤫𞤼𞤢𞤺𞤮𞤤', '𞤲𞤮', '𞤱𞤢𞤴𞤤𞤭𞤼𞤭𞤪𞤢', '𞤤𞤢𞥄𞤥𞤵', '𞤲𞤺𞤵𞤲', '.', '𞤮', '𞤻𞤢𞥄𞤱𞤢𞥄', '𞤮', '𞤬𞤢𞤱𞤢𞥄', '𞤱𞤢𞤪𞤫𞥅𞤣𞤫', '⹁', '𞤳𞤢𞤲𞤳𞤮', '𞤮', '𞤯𞤵𞥅𞤯𞤵𞤩𞤫', '.', '𞤢𞤸', '##𞤭𞥅𞤶𞤮', '𞤴𞤫𞤣𞥆𞤭', '𞤼𞤢𞤱𞤣𞤫𞥅𞤣𞤫', '𞤫', '𞤫𞤩𞥆𞤮𞥅𞤪𞤫', '𞤬𞤭𞥅', '𞤬𞤮𞤤𞥆𞤵𞤺𞤮𞤤', '𞤨𞤮𞤤', '𞤦𞤭', '##𞤴𞤢', '.', '𞤺𞤭𞤦𞤭', '𞤲𞤣𞤫𞤲', '⹁', '𞤢𞤸', '##𞤭𞥅𞤶𞤮', '𞤱𞤢𞥄𞤱𞤢𞥄𞤤𞤭', '𞤪𞤵𞤼𞥆𞤭𞤼𞤢𞥄𞤣𞤫', '𞤳𞤢', '##𞤥𞤢𞤪', '##𞤵𞤲', '𞤸𞤢𞥄', '𞤮', '𞤥𞤢𞥄𞤴𞤭', '𞤧𞤫𞤲𞤫𞤺𞤢𞤤', '𞤻𞤢𞤤𞤲𞤣𞤫', '𞥓𞥐', '𞤧𞤭𞤤', '##𞤼𞤮', '𞥑𞥙𞥘𞥙', '.', '𞤳𞤮', '𞤣', '##𞤢𞤳𞤢𞤪', '𞤳𞤢𞤣𞤭', '𞤮', '𞤵𞤦𞥆𞤢𞥄', '𞤳𞤢𞤲𞤳𞤮', '𞤫', '𞤺𞤫𞤲𞤣𞤭𞤪𞤢𞥄𞤸𞤮', '𞤥𞤢𞤳𞥆𞤮', '𞤬𞤫𞤰𞥆𞤭𞤲𞤯𞤮', '𞤫', '𞤧𞤫𞥅𞤯', '##𞤼𞤮', '𞥒𞥐𞥒𞥑', '.', '[SEP]']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xm/3ct03h_d797_ld5z2gg5nmsc0000gn/T/ipykernel_37912/986154554.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  tokenized_texts = torch.load('../cleaned_data/tokenized_articles.pt')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load tokenized texts\n",
    "tokenized_texts = torch.load('../cleaned_data/tokenized_articles.pt')\n",
    "print(type(tokenized_texts))\n",
    "print(tokenized_texts[:2])  # Print first two entries to inspect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adlamandnko-KnSx1iNo-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
